import numpy as np
import pandas as pd
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pickle
import os
from datetime import datetime, timedelta

class WaterPredictor:
    """Modelo de Red Neuronal Multicapa para predecir niveles de agua"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.is_trained = False
        self.model_path = "data/trained_model.pkl"
        
        # ‚úÖ INTENTAR CARGAR MODELO EXISTENTE AUTOM√ÅTICAMENTE
        self.load_model()
    
    def create_model(self):
        """Crear y configurar el modelo MLP"""
        self.model = MLPRegressor(
            hidden_layer_sizes=(64, 32, 16),  # 3 capas ocultas
            activation='relu',
            solver='adam',
            learning_rate='adaptive',
            max_iter=1000,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.2
        )
        print("‚úÖ Modelo MLP creado con arquitectura: 64-32-16")
    
    def generate_training_data(self, conn):
        """Generar datos de entrenamiento sint√©ticos basados en patrones realistas"""
        # En una implementaci√≥n real, esto vendr√≠a de la base de datos
        # Por ahora generamos datos sint√©ticos para demostraci√≥n
        
        np.random.seed(42)
        n_samples = 1000
        
        # Caracter√≠sticas (features)
        data = {
            'consumo_historico': np.random.normal(400, 100, n_samples),  # Consumo hist√≥rico
            'nivel_actual': np.random.uniform(10, 95, n_samples),        # Nivel actual
            'dias_sin_lluvia': np.random.randint(0, 30, n_samples),      # D√≠as sin lluvia
            'temperatura_promedio': np.random.normal(25, 5, n_samples),  # Temperatura
            'dia_semana': np.random.randint(0, 7, n_samples),            # D√≠a de la semana
            'es_fin_semana': np.random.choice([0, 1], n_samples),        # Es fin de semana
            'entregas_recientes': np.random.poisson(2, n_samples),       # Entregas recientes
        }
        
        df = pd.DataFrame(data)
        
        # Target: consumo futuro (basado en caracter√≠sticas con ruido)
        # F√≥rmula que simula relaciones reales
        df['consumo_futuro'] = (
            df['consumo_historico'] * 0.6 +
            (100 - df['nivel_actual']) * 2 +  # Mayor consumo cuando niveles bajos
            df['dias_sin_lluvia'] * 1.5 +     # M√°s consumo en sequ√≠a
            df['temperatura_promedio'] * 3 +  # M√°s consumo con calor
            df['es_fin_semana'] * 20 +        # M√°s consumo en fines de semana
            np.random.normal(0, 20, n_samples)  # Ruido
        )
        
        return df
    
    def prepare_features(self, df):
        """Preparar caracter√≠sticas para el modelo"""
        # Seleccionar features y target
        feature_columns = [
            'consumo_historico', 'nivel_actual', 'dias_sin_lluvia',
            'temperatura_promedio', 'dia_semana', 'es_fin_semana', 
            'entregas_recientes'
        ]
        
        X = df[feature_columns]
        y = df['consumo_futuro']
        
        return X, y
    
    def train(self, conn):
        """Entrenar el modelo con datos de la base de datos"""
        if self.model is None:
            self.create_model()
        
        print("üîÑ Generando datos de entrenamiento...")
        df = self.generate_training_data(conn)
        
        print("üîÑ Preparando caracter√≠sticas...")
        X, y = self.prepare_features(df)
        
        # Dividir datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Escalar caracter√≠sticas
        print("üîÑ Escalando caracter√≠sticas...")
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Entrenar modelo
        print("üîÑ Entrenando modelo MLP...")
        self.model.fit(X_train_scaled, y_train)
        
        # Evaluar modelo
        train_score = self.model.score(X_train_scaled, y_train)
        test_score = self.model.score(X_test_scaled, y_test)
        
        self.is_trained = True
        
        print(f"‚úÖ Modelo entrenado - Score entrenamiento: {train_score:.3f}")
        print(f"‚úÖ Modelo entrenado - Score prueba: {test_score:.3f}")
        
        # Guardar modelo
        self.save_model()
        
        return {
            "train_score": round(train_score, 3),
            "test_score": round(test_score, 3),
            "samples_used": len(df),
            "training_date": datetime.now().isoformat()
        }
    
    def predict(self, features_dict):
        """Hacer predicci√≥n con el modelo entrenado"""
        if not self.is_trained:
            # ‚úÖ INTENTAR CARGAR MODELO ANTES DE PREDECIR
            if not self.load_model():
                raise ValueError("Modelo no entrenado. Llama a train() primero.")
        
        # Convertir a DataFrame
        features_df = pd.DataFrame([features_dict])
        
        # Escalar caracter√≠sticas
        features_scaled = self.scaler.transform(features_df)
        
        # Hacer predicci√≥n
        prediction = self.model.predict(features_scaled)[0]
        
        return {
            "predicted_consumption": round(prediction, 2),
            "confidence": self._calculate_confidence(prediction),
            "timestamp": datetime.now().isoformat()
        }
    
    def _calculate_confidence(self, prediction):
        """Calcular confianza basada en la predicci√≥n (simplificado)"""
        # En una implementaci√≥n real, usar√≠amos probabilidades del modelo
        if prediction < 200:
            return 0.95  # Alta confianza para consumos bajos
        elif prediction < 500:
            return 0.85  # Media confianza para consumos moderados
        else:
            return 0.75  # Menor confianza para consumos altos
    
    def save_model(self):
        """Guardar modelo entrenado"""
        if not os.path.exists("data"):
            os.makedirs("data")
        
        with open(self.model_path, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'scaler': self.scaler,
                'is_trained': self.is_trained,
                'trained_date': datetime.now()
            }, f)
        print(f"‚úÖ Modelo guardado en {self.model_path}")
    
    def load_model(self):
        """Cargar modelo entrenado"""
        if os.path.exists(self.model_path):
            try:
                with open(self.model_path, 'rb') as f:
                    saved_data = pickle.load(f)
                
                self.model = saved_data['model']
                self.scaler = saved_data['scaler']
                self.is_trained = saved_data['is_trained']
                print("‚úÖ Modelo cargado desde archivo")
                return True
            except Exception as e:
                print(f"‚ùå Error cargando modelo: {e}")
                return False
        else:
            print("‚ÑπÔ∏è No se encontr√≥ modelo guardado")
            return False

def ensure_trained(conn):
    """Asegurar que el modelo est√© entrenado - Cargar o entrenar"""
    # Primero intentar cargar modelo existente
    if water_predictor.load_model():
        return {"status": "loaded_existing", "message": "Modelo cargado desde archivo"}
    
    # Si no existe, entrenar nuevo modelo
    print("üîÑ Modelo no encontrado - Entrenando nuevo modelo...")
    result = water_predictor.train(conn)
    return {"status": "newly_trained", "message": "Modelo entrenado exitosamente", "results": result}

# Instancia global del predictor
water_predictor = WaterPredictor()